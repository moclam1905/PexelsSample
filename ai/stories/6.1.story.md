# Story 6.1: Implement Core Pinch-to-Zoom and Pan Functionality

**Status:** Done

## Goal & Context

**User Story:** As a user, when viewing an image in the detail screen, I want to use pinch gestures to zoom in and out, and drag gestures to pan the zoomed image, so I can inspect image details more closely.

**Context:** This story is the first in Epic 6 (Advanced Image Interaction) and builds upon the completed `ImageDetailScreen` from Epic 4. It focuses on integrating the fundamental gesture detection and image transformation logic using Jetpack Compose's native capabilities, as researched in `docs/deep-research-bonus-features.md`.

## Detailed Requirements

* Integrate gesture detection within the `ImageDetailScreen` Composable (likely around the `AsyncImage` Composable displaying the full image).
* Utilize `Modifier.pointerInput` with `detectTransformGestures` for handling pinch (scale) and drag (pan) events.
* Maintain state for current `scale`, `offsetX`, and `offsetY` using `remember` and `mutableStateOf`.
* Apply transformations to the `AsyncImage` Composable using `Modifier.graphicsLayer { scaleX = currentScale; scaleY = currentScale; translationX = currentOffsetX; translationY = currentOffsetY; }`.
* Ensure that panning is only effective when the `currentScale` makes the image content larger than the view bounds.
* The initial implementation will focus on these Compose-native capabilities.

## Acceptance Criteria (ACs)

* ✅ AC1: Users can zoom into an image in the detail view using a two-finger pinch-out gesture, updating the `scale` state.
* ✅ AC2: Users can zoom out of an image in the detail view using a two-finger pinch-in gesture, updating the `scale` state.
* ✅ AC3: When an image is zoomed in (`scale` > fit-to-view scale), users can pan the image horizontally and vertically by dragging, updating `offsetX` and `offsetY` states.
* ✅ AC4: Zooming and panning actions feel smooth and responsive on target devices/emulators.
* ✅ AC5: Panning is restricted if the scaled image's dimension is smaller than or equal to the view bounds in that axis.
* ✅ AC6: The gesture detection mechanism is primarily based on Jetpack Compose's `pointerInput` and `detectTransformGestures`.

## Technical Implementation Context

**Guidance:** Use the following details for implementation. Refer to the linked `docs/` files for broader context if needed. Epic 4 (Image Detail View) is complete.

* **Relevant Files:**
    * Files to Modify:
        * `app/src/main/java/com/nguyenmoclam/pexelssample/ui/detail/ImageDetailScreen.kt`
    * Files to Create: None for this story.
    * _(Hint: See `docs/project-structure.md` for `ui/detail/` package. Refer to `docs/deep-research-bonus-features.md` [159-163] for native gesture detection and `docs/coding-standards.md` for gesture handling in Compose.)_

* **Key Technologies:**
    * Jetpack Compose: `Modifier.pointerInput`, `detectTransformGestures`, `Modifier.graphicsLayer`, `remember`, `mutableStateOf`.
    * `AsyncImage` (from Coil, already used in Epic 4).
    * Kotlin for state management and gesture logic.
    * _(Hint: See `docs/tech-stack.md` for Compose versions.)_

* **API Interactions / SDK Usage:**
    * Not applicable for this story.
    * _(Hint: See `docs/api-reference.md`)_

* **Data Structures:**
    * Local Composable states for `scale: Float`, `offsetX: Float`, `offsetY: Float`.
    * _(Hint: See `docs/data-models.md`)_

* **Environment Variables:**
    * Not applicable for this story.
    * _(Hint: See `docs/environment-vars.md`)_

* **Coding Standards Notes:**
    * Gesture state (`scale`, `offsetX`, `offsetY`) should be managed using `remember` and `mutableStateOf`.
    * Transformations applied via `Modifier.graphicsLayer` for efficiency.
    * Panning logic should consider current scale and view bounds.
    * Follow gesture handling best practices outlined in `docs/coding-standards.md` (new section).
    * Refer to `docs/deep-research-bonus-features.md` [161-164] for insights on native implementation.
    * _(Hint: See `docs/coding-standards.md` for full standards.)_

## Tasks / Subtasks

* [ ] In `ImageDetailScreen.kt`, identify the `AsyncImage` Composable that displays the full image.
* [ ] Initialize state variables for scale, offsetX, and offsetY:
    ```kotlin
    var scale by remember { mutableStateOf(1f) }
    var offsetX by remember { mutableStateOf(0f) }
    var offsetY by remember { mutableStateOf(0f) }
    // These will be refined with initial fit-to-view scale in Story 6.2
    ```
* [ ] Apply `Modifier.pointerInput(Unit) { detectTransformGestures { centroid, pan, zoom, rotation -> ... } }` to the `AsyncImage` or its direct interactive parent.
    * Inside `detectTransformGestures`:
        * Update `scale *= zoom`. (Clamping will be added in Story 6.2).
        * Update `offsetX += pan.x`.
        * Update `offsetY += pan.y`. (Boundary checks for pan will be added in Story 6.2).
        * Ignore `rotation` and `centroid` for now, or use `centroid` if attempting centered zoom.
* [ ] Apply transformations using `Modifier.graphicsLayer`:
    ```kotlin
    AsyncImage(
        // ... existing parameters ...
        modifier = Modifier
            // ... other existing modifiers ...
            .graphicsLayer(
                scaleX = scale,
                scaleY = scale,
                translationX = offsetX,
                translationY = offsetY
            )
            .pointerInput(Unit) { /* ... detectTransformGestures ... */ }
    )
    ```
* [ ] Implement basic logic to restrict panning if not zoomed enough (AC5): This means if `scale <= 1f` (initial approximate logic before precise fit-to-view scale), `pan.x` and `pan.y` changes might be ignored or reset `offsetX`/`offsetY` to 0. This will be refined in Story 6.2 with proper bounds.
* [ ] Test basic pinch-to-zoom and pan functionality on an emulator/device.

## Testing Requirements

**Guidance:** Verify implementation against the ACs using the following tests.

* **Unit Tests:**
    * Not primary for direct gesture interaction logic within a Composable. Logic for calculating transformations, if complex and extracted, could be unit tested.
* **Integration Tests (UI Tests using Jetpack Compose UI Test):**
    * (To be formalized later) Simulate multi-touch gestures (pinch, pan) using test framework capabilities.
    * Assert that `scale`, `offsetX`, `offsetY` states are updated as expected.
    * Verify `Modifier.graphicsLayer` applies the transformations visually (may require screenshot testing or validating transformation matrix if possible).
    * _(Refer to `docs/testing-strategy.md` [Epic 6 points] for UI test considerations for gestures.)_
* **Manual/CLI Verification:**
    * AC1: Run app, navigate to `ImageDetailScreen`. Use two fingers to pinch out; verify the image scales up.
    * AC2: Use two fingers to pinch in; verify the image scales down.
    * AC3: Zoom into an image significantly. Use one finger to drag; verify the image pans.
    * AC4: Perform zoom and pan gestures repeatedly. Observe for smoothness and responsiveness.
    * AC5: When image is at its initial (or near initial) scale, attempt to pan. Verify it doesn't pan or has very limited movement.
    * AC6: Review code to confirm `Modifier.pointerInput` and `detectTransformGestures` are the core mechanisms.
* _(Hint: See `docs/testing-strategy.md` and relevant research in `docs/deep-research-bonus-features.md` [179, 181-182].)_

